---
title: "Stacked Ensembles of H2O and XGBoost Models"
author: "Erin LeDell"
date: "4/25/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


![](https://media.giphy.com/media/Qh7QVjnRgP040/giphy.gif)

<p><a href="http://giphy.com/gifs/Qh7QVjnRgP040">via GIPHY</a></p>


This tutorial will demonstrate how to use the **h2o** R package to combine H2O models with XGBoost models into a Stacked Ensemble.



## Install XGBoost-enabled H2O

Currently, XGBoost is available in a special development edition of H2O, available (temporarily) [here](http://www.stat.berkeley.edu/~ledell/files/h2o-3.11.0.99999.zip).  Download the file, unzip it, and `cd` to the `./R/` directory.  Install the R package: `R CMD install ./R/h2o_3.11.0.99999.tar.gz`

H2O ships with everything -- except the system library for multithreading (openMP).  If you are on a Mac, you will need to install OpenMP.

### Mac

```bash
# Install OpenMP (required of xgboost-enabled h2o)
brew install gcc --without-multilib
```

For now, let's assume the h2o (+xgb) R package one is installed currently.




## Train Base Learners

Let's train and cross-validate a set of H2O and XGBoost models and then create a [Stacked Ensemble](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/stacked-ensembles.html) using the h2o R package.

### Start H2O Cluster & Load Data

```{r h2o_load_data}
library(h2o)
h2o.init(nthreads = -1)
h2o.no_progress() # Don't show progress bars in RMarkdown output

# Import a sample binary outcome train/test set into H2O
train <- h2o.importFile("https://s3.amazonaws.com/erin-data/higgs/higgs_train_10k.csv")
test <- h2o.importFile("https://s3.amazonaws.com/erin-data/higgs/higgs_test_5k.csv")

# Identify predictors and response
y <- "response"
x <- setdiff(names(train), y)

# For binary classification, response should be a factor
train[,y] <- as.factor(train[,y])
test[,y] <- as.factor(test[,y])

# Number of CV folds (to generate level-one data for stacking)
nfolds <- 5
```

### H2O base models

```{r h2o_train_cv}
# Train & Cross-validate a GBM
my_gbm <- h2o.gbm(x = x,
                  y = y,
                  training_frame = train,
                  distribution = "bernoulli",
                  ntrees = 10,
                  max_depth = 3,
                  min_rows = 2,
                  learn_rate = 0.2,
                  nfolds = nfolds,
                  fold_assignment = "Modulo",
                  keep_cross_validation_predictions = TRUE,
                  seed = 1)

# Train & Cross-validate a RF
my_rf <- h2o.randomForest(x = x,
                          y = y,
                          training_frame = train,
                          ntrees = 50,
                          nfolds = nfolds,
                          fold_assignment = "Modulo",
                          keep_cross_validation_predictions = TRUE,
                          seed = 1)


# Train & Cross-validate a DNN
my_dl <- h2o.deeplearning(x = x,
                          y = y,
                          training_frame = train,
                          nfolds = nfolds,
                          fold_assignment = "Modulo",
                          keep_cross_validation_predictions = TRUE,
                          seed = 1)

```



### XGBoost base models

```{r xgb_train_cv}
# Train & Cross-validate a XGB-GBM
my_xgb1 <- h2o.xgboost(x = x,
                       y = y,
                       training_frame = train,
                       distribution = "bernoulli",
                       ntrees = 100,
                       max_depth = 3,
                       min_rows = 2,
                       learn_rate = 0.2,
                       nfolds = nfolds,
                       fold_assignment = "Modulo",
                       keep_cross_validation_predictions = TRUE,
                       seed = 1)
```


## Create a Stacked Ensemble

To maximize predictive power, will create an [H2O Stacked Ensemble](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/stacked-ensembles.html) from the models we created above and print the performance gain the ensemble has over the best base model.

```{r create_ensemble}
# Train a stacked ensemble using the H2O and XGBoost models from above
base_models <- list(my_gbm@model_id, my_rf@model_id, my_dl@model_id, my_xgb1@model_id)

ensemble <- h2o.stackedEnsemble(x = x,
                                y = y,
                                training_frame = train,
                                model_id = "h2o_xgb_ensemble",
                                base_models = base_models)

# Eval ensemble performance on a test set
perf <- h2o.performance(ensemble, newdata = test)


# Compare to base learner performance on the test set
perf_gbm_test <- h2o.performance(my_gbm, newdata = test)
perf_rf_test <- h2o.performance(my_rf, newdata = test)
perf_dl_test <- h2o.performance(my_dl, newdata = test)
# TO DO: Fix this bug, right now h2o.performance does not work on xgb models
#perf_xgb1_test <- h2o.performance(my_xgb1, newdata = test) 
#Error in Filter(function(mm) { : subscript out of bounds  
baselearner_best_auc_test <- max(h2o.auc(perf_gbm_test), 
                                 h2o.auc(perf_rf_test),
                                 h2o.auc(perf_dl_test))#,
#                                 h2o.auc(perf_xgb1_test))
ensemble_auc_test <- h2o.auc(perf)
print(sprintf("Best Base-learner Test AUC:  %s", baselearner_best_auc_test))
print(sprintf("Ensemble Test AUC:  %s", ensemble_auc_test))
```
