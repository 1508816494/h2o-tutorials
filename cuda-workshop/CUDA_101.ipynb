{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This is an introductory, hands-on, workshop for programming GPU devices using [CUDA](https://developer.nvidia.com/cuda-zone) C/C++. Even though we will be using a Jupyter notebook to edit and run our code be adviced that none of the commands are actually executed here but rather on the server (be it one of the cloud instances or your local machine).\n",
    "\n",
    "In this lab we will:\n",
    "\n",
    "1. see a brief overview of CPU vs GPU architectures\n",
    "2. discuss why GPUs are better for certain tasks often utilized in machine learning and what are the limitations\n",
    "3. explain how to extend C/C++ code using CUDA so it can be executed on GPUS\n",
    "4. write simple GPU enabled programs\n",
    "5. mention several higher level libraries, which will make you more productive\n",
    "\n",
    "---\n",
    "\n",
    "For people not acquainted with how Jupyter Notebook works, every notebook consists of a set of *cells*, each having a type. Each cell can be executed by navigating to it (e.g. by clicking it) and pressing **ctrl+enter** (or **cmd+enter** on macbooks) or by clicking the \"Run cell\" icon in the top menu.\n",
    "\n",
    "Try and run the below cell (you can run terminal commands in a *code* cell by prepending them with an exclamation **!** mark). The output, containing information about GPU devices running on the server, should appear below it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPU vs GPU Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![cpu-gpu-transistors](gpu-devotes-more-transistors-to-data-processing.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CPU**\n",
    "* few cores optimized for *serial processing*\n",
    "* lower memory bandwith (but direct access to more memory) \n",
    "* *latency* optimized cores (faster at a single task but can only perform few at the same time)\n",
    "* more instructions but slower execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GPU**\n",
    "* hundreds/thousands of smaller, more efficient cores optimized for *multiple tasks simultaneously*\n",
    "* great for compute-intensive parts of the application\n",
    "* higher memory bandwidth (but direct access to less memory than CPU)\n",
    "* *throughput* optimized cores (slower at single tasks but can perform more at the same time)\n",
    "* limited number of instructions available but faster execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![how-gpu-acceleration-works](how-gpu-acceleration-works.png)\n",
    "*http://www.nvidia.com/object/what-is-gpu-computing.html\n",
    "\n",
    "In a GPU accelerated appplication the compute-intensive parts are offloaded to the GPU, while the remaining of the application runs on the CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPUs in Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pros**\n",
    "* ML algorithms tend to be **compute-bound** (require more computations than fetches from memory) - GPUs are intended for compute intensive tasks!\n",
    "* many ML algorithms consist of highly parallelizable tasks e.g. matrix multiplications peformed over multiple iterations - GPUs are great at those\n",
    "\n",
    "**Drawbacks**\n",
    "* host RAM to GPU RAM memory copy overhead (over PCI-E)\n",
    "* limited (very limited compared to host RAM) GPU RAM size\n",
    "* serial tasks slow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CUDA Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will outline the main concepts behind the CUDA programming model (focusing on how they are exposed in C). A more in-depth explanation can be found in the [official programming guide](http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html).\n",
    "\n",
    "**host** - CPU and RAM\n",
    "\n",
    "**device** - GPU and it's RAM\n",
    "\n",
    "**kernels** - special functions, which can be called from host code (regular C code running on the CPU) but are run on the device (GPU) N times in parallel, executed by N CUDA threads.\n",
    "\n",
    "**\\__global__** - special CUDA C keyword used as part of a method signature to mark that method as a kernel.\n",
    "\n",
    "**\\__host__** - methods marked with this keyword can be only called from host code and will also run on the host\n",
    "\n",
    "**\\__device__** - methods marked with this keyword can be only called from device code and will also run on the device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![grid-of-thread-blocks](grid-of-thread-blocks.png)\n",
    "\n",
    "**thread block** - CUDA programming model follows a well defined thread hierarchy model in which threads during execution are grouped into so called thread blocks. Thread blocks can be one, two or three dimensional. This very naturally maps to vectors, matrices and volumes.\n",
    "\n",
    "There's a limit of threads per block since 1) all the threads in a block are expected to reside on the same processor core 2) have to share very limited resources of said core.\n",
    "\n",
    "We can run a kernel, though, using multiple blocks each with the same number of threads. The total number of threads per kernel in such a case will be number_of_blocks * number_of_threads_per_block.\n",
    "\n",
    "**threadIdx.{x,y,z}** - similar to the above these variables identify the thread ID that is being executed with the current block.\n",
    "\n",
    "**blockIdx.{x,y,z}** - these 3 special, read-only, variables can be used within CUDA kernels to find out which block is executing the code at a given time. This, and the following read-only variables, are usually used to identify which part of data should be handled by a given kernel instance as there will be N of them running in parallel.\n",
    "\n",
    "**blockDim.{x,y,z}** - another special read-only variable accesible within CUDA kernels. Contrary to the previous ones these are constant and describe the total number of threads within a block.\n",
    "\n",
    "**grid** - similarly to threads being organized in thread blocks, thread blocks are organized into one, two or three dimensional grids. The number of blocks is dictated by the size of data and/or number of processors available.\n",
    "\n",
    "**methodName <<< blocks, threads_per_block>>> (parameters)** - this is how you launch a kernel. You use this syntax in your host code (regular C code which runs on the CPU) but it is executed on the device (GPU). There are numerous variants of this operator but the most basic one takes in 2 numbers: the number of blocks to run on the GPU and the second the number of threads per each block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 0: Hello CUDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets start with the most basic C program there is, a \"Hello, World!\". We defined one in the [lab0/lab0a.c](/edit/lab0/lab0a.c) file. You can run terminal commans from this notebook by prepending them with an exclamation mark **!**. The command below will compile the C file using **gcc**, which will output the **lab0/lab0a.out** binary and subsequently run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!gcc lab0/lab0a.c -o lab0/lab0a.out && lab0/lab0a.out "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets modify this code so it can run on the GPU. The modified version can be found here [lab0/lab0b.cu](/edit/lab0/lab0b.cu) (.cu being the usual suffix for CUDA source files). As you can see several things changed:\n",
    "\n",
    "1. the **hello** function has been prefixed with the **__global__** keyword making it a kernel\n",
    "2. thanks to the above we can call threadIdx.x and blockIdx.x (and other variables discussed before) to get additional information on the context in which the kernel is being ran\n",
    "3. the call to the *hello* method has been changed to the **<<<...>>>** syntax - in this case we will run it only using 1 block and 1 thread.\n",
    "4. a call to **cudaDeviceSynchronize()** has been added as all calls to kernels are **asynchronous**, meaning our main method might return before anything got actually printed. This method is blocking and makes sure all the operations on the GPU finish before it returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!nvcc -o lab0/lab0b.out lab0/lab0b.cu -run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try** changing the values (modify the file, save it and rerun the above cell) between **<<<...>>>** to:\n",
    "\n",
    "- **<<< 2, 1 >>>**\n",
    "- **<<< 1, 32 >>>** how did the output change? Is the order what you'd expect?\n",
    "- **<<< 2, 16 >>>** can you set those numbers much higher?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Lab 1: Adding arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you know what kernels are, how to write and launch them lets try to write one which adds two arrays together storing the result in a 3rd array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already prepared all the boilerplate code for you in [lab1/lab1a.cu](/edit/lab1/lab1a.cu) so please just focus on the parts marked by TODO comments. You will notice certain pieces of code in the main method which we will cover in later labs, please ignore them for now.\n",
    "\n",
    "You'll have to figure out 3 things:\n",
    "\n",
    "- modify a regular C signature into a CUDA kernel\n",
    "- use the read-only CUDA provided variables (described in the CUDA overview section) to calculate the index for each kernel instance\n",
    "- in our example we will have number_of_blocks * threads_per_block instances, what if this number if bigger than the sizes of our arrays? You will need to sanity check this\n",
    "\n",
    "After modifying the file be sure to save it either by pressing **ctrl+s** (**cmd+s**) or using the top menu File -> Save. When you're done run the cell below. Your result should show all 0s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!nvcc -o lab1/lab1a.out lab1/lab1a.cu -run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're having problems you can have a look at the solution [lab1/lab1a_solution.cu](/edit/lab1/lab1a_solution.cu)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thrust"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quoting the official [Thrust website](https://developer.nvidia.com/thrust):\n",
    "\n",
    "> Thrust is a powerful library of parallel algorithms and data structures. Thrust provides a flexible, high-level interface for GPU programming that greatly enhances developer productivity. Using Thrust, C++ developers can write just a few lines of code to perform GPU-accelerated sort, scan, transform, and reduction operations orders of magnitude faster than the latest multi-core CPUs.\n",
    "\n",
    "Thrust follows very closely the [C++ Standard Template Library](http://www.cplusplus.com/reference/stl/) API and approach exposing high level access to many useful containers and algorithms. It makes moving data between host and device memories very easy and provided algorithms are highly optimized for GPUs (taking into account things like nondeterministic order of operations and working with floating point precision numbers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In [lab1/lab1b.cu](/edit/lab1/lab1b.cu) we use [Thrust vectors](http://docs.nvidia.com/cuda/thrust/index.html#vectors) for the same problem as before. Thrust device_vector makes it easy to allocate memory on the device and assign values. Moving data back to host memory is easily accomplished by declaring host_vector and assigning device_vector to it.\n",
    "\n",
    "In this exercise we want to use one of the [Thrust transformation methods](https://thrust.github.io/doc/group__transformations.html). We can think about vector addition as a transformation of 2 iterators where we sum appropriate elements of each vector and put the result into the output vector.\n",
    "\n",
    "After modifying the file you can execute the cell below to run it. You should get the same output as in the previous exercise. All necessary includes are already in the file, focus just on the TODO part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!nvcc -o lab1/lab1b.out lab1/lab1b.cu -run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Hint1](/edit/lab1/hint1.txt)\n",
    "\n",
    "[Hint2](/edit/lab1/hint2.txt)\n",
    "\n",
    "If you still find it challenging you can check the [solution here](/edit/lab1/lab1b_solution.cu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2: Matrix multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cuBLAS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: Error handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4: Memory management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5: Thread synchronization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
