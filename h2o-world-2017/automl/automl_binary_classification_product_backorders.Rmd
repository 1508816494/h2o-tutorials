---
title: "H2O AutoML Binary Classification Demo"
output: html_notebook
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. To execute a code chunk, click *Run* (play) button within the chunk or by placing your cursor inside it and pressing *Cmd+Shift+Enter*. 


### Start H2O

Load the **h2o** R library and initiatize a local H2O cluster.

```{r}
library(h2o)
h2o.init()
h2o.no_progress()  # Turn off progress bars for notebook readability
```

### Load Data

```{r}
# Import a binary classfication dataset
# We will use a subset of the Product Backorders dataset
# Source: https://www.kaggle.com/haimfeld87/predict-product-backorders-with-smote-and-rf/data
df <- h2o.importFile("/Users/me/h2oai/code/h2o-tutorials/h2o-world-2017/automl/product_backorders.csv")
#df <- h2o.importFile("/home/h2o/data/automl/product_backorders.csv")
```


For classification, the response should be encoded as categorical (aka. "factor" or "enum"). Let's take a look.
```{r}
h2o.describe(df)
```
We will notice that it's already encoded as "enum", so there's nothing we need to do here.  If it were encoded as a 0/1 "int", then we'd have to convert the colum as follows:  `df[,y] <- as.factor[,y]`


Next, let's identify the response & predictor columns by saving them as `x` and `y`.
```{r}
y <- "went_on_backorder"
x <- setdiff(names(df), c(y, "sku"))
```


## Run AutoML 

Run AutoML, stopping after 10 models.  The `max_models` argument specifies the number of individual (or "base") models, and does not include the two ensemble models that are trained at the end.
```{r}
aml <- h2o.automl(y = y, x = x,
                  training_frame = df,
                  max_models = 10,
                  seed = 1)
```


## Leaderboard

Next, we will view the AutoML Leaderboard.  Since we did not specify a `leaderboard_frame` in the `h2o.automl()` function for scoring and ranking the models, the AutoML leaderboard uses cross-validation metrics to rank the models.

The leaderboard is stored here:
```{r}
lb <- aml@leaderboard
```

Now we will view a snapshot of the top models.  Here we should see the two Stacked Ensembles at or near the top of the leaderboard.  Stacked Ensembles almost always outperform a single model.
```{r}
print(lb)
```

To view the entire leaderboard, specify the `n` argument of the `print.H2OFrame()` function as the total number of rows:
```{r}
print(lb, n = nrow(lb))
```

The leader model is stored here:
```{r}
aml@leader
```


## Ensemble Exploration

To understand how the ensembles work, let's take a peek inside the Stacked Ensemble "All Models" model.


```{r}
# Get model ids for all models in the AutoML Leaderboard
model_ids <- as.data.frame(aml@leaderboard$model_id)[,1]
# Get the Stacked Ensemble model
se <- h2o.getModel(grep("StackedEnsemble_AllModels", model_ids, value = TRUE)[1])
# Get the Stacked Ensemble metalearner model
meta <- h2o.getModel(se@model$metalearner$name)
```

Examine the variable importance of the metalearner (combiner) algorithm in the ensemble.  This shows us how much each base learner is contributing to the ensemble. The AutoML Stacked Ensembles use the default metalearner algorithm (GLM with non-negative weights), so the variable importance of the metaleraner is actually the standardized coefficeint magnitudes of the GLM. 
```{r}
h2o.varimp(meta)
```

We can also plot the base learner contributions to the ensemble.
```{r}
h2o.varimp_plot(meta)
```

## Save Leader Model
There are two ways to save the leader model -- binary format and MOJO format.  If you're taking your leader model to production, then we'd suggest the MOJO format since it's optimized for production use.
```{r}
h2o.saveModel(aml@leader, path = "./product_backorders_model_bin")
```

```{r}
h2o.download_mojo(aml@leader, path = "./")
```

