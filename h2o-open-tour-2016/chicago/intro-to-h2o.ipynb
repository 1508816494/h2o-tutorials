{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introductory H2O Machine Learning Tutorial\n",
    "\n",
    "Prepared for H2O Open Chicago 2016: http://open.h2o.ai/chicago.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install H2O\n",
    "\n",
    "The first step in this tutorial is to download and install the h2o Python module.  \n",
    "The latest version is always here: http://www.h2o.ai/download/h2o/py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start up the H2O Cluster\n",
    "\n",
    "Once the Python module is installed, we begin by starting up a local (on your laptop) H2O cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "No instance found at ip and port: localhost:54321. Trying to start local jar...\n",
      "\n",
      "\n",
      "JVM stdout: /var/folders/2j/jg4sl53d5q53tc2_nzm9fz5h0000gn/T/tmpfsL1IA/h2o_me_started_from_python.out\n",
      "JVM stderr: /var/folders/2j/jg4sl53d5q53tc2_nzm9fz5h0000gn/T/tmpzl_9R7/h2o_me_started_from_python.err\n",
      "Using ice_root: /var/folders/2j/jg4sl53d5q53tc2_nzm9fz5h0000gn/T/tmpYBAmbc\n",
      "\n",
      "\n",
      "Java Version: java version \"1.8.0_45\"\n",
      "Java(TM) SE Runtime Environment (build 1.8.0_45-b14)\n",
      "Java HotSpot(TM) 64-Bit Server VM (build 25.45-b02, mixed mode)\n",
      "\n",
      "\n",
      "Starting H2O JVM and connecting: ......... Connection successful!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/IPython/core/formatters.py:92: DeprecationWarning: DisplayFormatter._ipython_display_formatter_default is deprecated: use @default decorator instead.\n",
      "  def _ipython_display_formatter_default(self):\n",
      "/usr/local/lib/python2.7/site-packages/IPython/core/formatters.py:98: DeprecationWarning: DisplayFormatter._formatters_default is deprecated: use @default decorator instead.\n",
      "  def _formatters_default(self):\n",
      "/usr/local/lib/python2.7/site-packages/IPython/core/formatters.py:677: DeprecationWarning: PlainTextFormatter._deferred_printers_default is deprecated: use @default decorator instead.\n",
      "  def _deferred_printers_default(self):\n",
      "/usr/local/lib/python2.7/site-packages/IPython/core/formatters.py:669: DeprecationWarning: PlainTextFormatter._singleton_printers_default is deprecated: use @default decorator instead.\n",
      "  def _singleton_printers_default(self):\n",
      "/usr/local/lib/python2.7/site-packages/IPython/core/formatters.py:672: DeprecationWarning: PlainTextFormatter._type_printers_default is deprecated: use @default decorator instead.\n",
      "  def _type_printers_default(self):\n",
      "/usr/local/lib/python2.7/site-packages/IPython/core/formatters.py:669: DeprecationWarning: PlainTextFormatter._singleton_printers_default is deprecated: use @default decorator instead.\n",
      "  def _singleton_printers_default(self):\n",
      "/usr/local/lib/python2.7/site-packages/IPython/core/formatters.py:672: DeprecationWarning: PlainTextFormatter._type_printers_default is deprecated: use @default decorator instead.\n",
      "  def _type_printers_default(self):\n",
      "/usr/local/lib/python2.7/site-packages/IPython/core/formatters.py:677: DeprecationWarning: PlainTextFormatter._deferred_printers_default is deprecated: use @default decorator instead.\n",
      "  def _deferred_printers_default(self):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O cluster uptime: </td>\n",
       "<td>1 seconds 115 milliseconds </td></tr>\n",
       "<tr><td>H2O cluster version: </td>\n",
       "<td>3.8.2.3</td></tr>\n",
       "<tr><td>H2O cluster name: </td>\n",
       "<td>H2O_started_from_python_me_ybz837</td></tr>\n",
       "<tr><td>H2O cluster total nodes: </td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O cluster total free memory: </td>\n",
       "<td>7.11 GB</td></tr>\n",
       "<tr><td>H2O cluster total cores: </td>\n",
       "<td>8</td></tr>\n",
       "<tr><td>H2O cluster allowed cores: </td>\n",
       "<td>8</td></tr>\n",
       "<tr><td>H2O cluster healthy: </td>\n",
       "<td>True</td></tr>\n",
       "<tr><td>H2O Connection ip: </td>\n",
       "<td>127.0.0.1</td></tr>\n",
       "<tr><td>H2O Connection port: </td>\n",
       "<td>54321</td></tr>\n",
       "<tr><td>H2O Connection proxy: </td>\n",
       "<td>None</td></tr>\n",
       "<tr><td>Python Version: </td>\n",
       "<td>2.7.10</td></tr></table></div>"
      ],
      "text/plain": [
       "------------------------------  ---------------------------------\n",
       "H2O cluster uptime:             1 seconds 115 milliseconds\n",
       "H2O cluster version:            3.8.2.3\n",
       "H2O cluster name:               H2O_started_from_python_me_ybz837\n",
       "H2O cluster total nodes:        1\n",
       "H2O cluster total free memory:  7.11 GB\n",
       "H2O cluster total cores:        8\n",
       "H2O cluster allowed cores:      8\n",
       "H2O cluster healthy:            True\n",
       "H2O Connection ip:              127.0.0.1\n",
       "H2O Connection port:            54321\n",
       "H2O Connection proxy:\n",
       "Python Version:                 2.7.10\n",
       "------------------------------  ---------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the H2O library and start up the H2O cluter locally on your machine\n",
    "import h2o\n",
    "\n",
    "# Number of threads, nthreads = -1, means use all cores on your machine\n",
    "# max_mem_size is the maximum memory (in GB) to allocate to H2O\n",
    "h2o.init(nthreads = -1, max_mem_size = 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data prep\n",
    "\n",
    "### Import data\n",
    "Next we will import a cleaned up version of the Lending Club \"Bad Loans\" dataset. The purpose here is to predict whether a loan will be bad (i.e. not repaid to the lender). The response column, `bad_loan`, is 1 if the loan was bad, and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parse Progress: [##################################################] 100%\n"
     ]
    }
   ],
   "source": [
    "loan_csv = \"/Users/me/h2oai/code/demos/lending_club/loan.csv\"  # modify this for your machine\n",
    "# Alternatively, you can import the data directly from a URL\n",
    "#loan_csv = \"https://raw.githubusercontent.com/h2oai/app-consumer-loan/master/data/loan.csv\"\n",
    "data = h2o.import_file(loan_csv)  # 163,994 rows x 15 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(163994, 15)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode response variable\n",
    "Since we want to train a binary classification model, we must ensure that the response is coded as a factor. If the response is 0/1, H2O will assume it's numeric, which means that H2O will train a regression model instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['0', '1']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['bad_loan'] = data['bad_loan'].asfactor()  #encode the binary repsonse as a factor\n",
    "data['bad_loan'].levels()  #optional: after encoding, this shows the two factor levels, '0' and '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partition data\n",
    "\n",
    "Next, we partition the data into training, validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Partition data into 70%, 15%, 15% chunks\n",
    "# Setting a seed will guarantee reproducibility\n",
    "\n",
    "splits = data.split_frame(ratios=[0.7, 0.15], seed=1)  \n",
    "\n",
    "train = splits[0]\n",
    "valid = splits[1]\n",
    "test = splits[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that `split_frame()` uses approximate splitting not exact splitting (for efficiency), so these are not exactly 70%, 15% and 15% of the total rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114914\n",
      "24499\n",
      "24581\n"
     ]
    }
   ],
   "source": [
    "print train.nrow\n",
    "print valid.nrow\n",
    "print test.nrow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify response and predictor variables\n",
    "In H2O, we use `y` to designate the response variable and `x` to designate the list of predictor columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = 'bad_loan'\n",
    "x = list(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x.remove(y)  #remove the response\n",
    "x.remove('int_rate')  #remove the interest rate column because it's correlated with the outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'loan_amnt',\n",
       " u'term',\n",
       " u'emp_length',\n",
       " u'home_ownership',\n",
       " u'annual_inc',\n",
       " u'verification_status',\n",
       " u'purpose',\n",
       " u'addr_state',\n",
       " u'dti',\n",
       " u'delinq_2yrs',\n",
       " u'revol_util',\n",
       " u'total_acc',\n",
       " u'longest_credit_length']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of predictor columns\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## H2O Machine Learning\n",
    "\n",
    "Now that we have prepared the data, we can train some models. We will start by training a single model from each of the H2O supervised algos:\n",
    "\n",
    "- Generalized Linear Model (GLM)\n",
    "- Random Forest (RF)\n",
    "- Gradient Boosting Machine (RF)\n",
    "- Deep Learning (DL)\n",
    "- Naive Bayes (NB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generalized Linear Model\n",
    "Let's start with a basic binomial Generalized Linear Model (GLM).  By default, H2O's GLM uses a regularized, elastic net model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import H2O GLM:\n",
    "from h2o.estimators.glm import H2OGeneralizedLinearEstimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a default GLM\n",
    "We first create an object of class, `\"H2OGeneralizedLinearEstimator\"`.  This does not actually do any training, it just sets the model up for training by specifying model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize the GLM estimator:\n",
    "# Similar to R's glm() and H2O's R GLM, H2O's GLM has the \"family\" argument\n",
    "\n",
    "glm_fit1 = H2OGeneralizedLinearEstimator(family='binomial', model_id='glm_fit1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that `glm_fit1` object is initialized, we can train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "glm Model Build Progress: [##################################################] 100%\n"
     ]
    }
   ],
   "source": [
    "glm_fit1.train(x=x, y=y, training_frame=train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a GLM with lambda search\n",
    "\n",
    "Next we will do some automatic tuning by passing in a validation frame and setting `lambda_search = True`.  Since we are training a GLM with regularization, we should try to find the right amount of regularization (to avoid overfitting).  The model parameter, `lambda`, controls the amount of regularization in a GLM model and we can find the optimal value for `lambda` automatically by setting `lambda_search = True` and passing in a validation frame (which is used to evaluate model performance using a particular value of lambda)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "glm Model Build Progress: [##################################################] 100%\n"
     ]
    }
   ],
   "source": [
    "glm_fit2 = H2OGeneralizedLinearEstimator(family='binomial', model_id='glm_fit2', lambda_search=True)\n",
    "glm_fit2.train(x=x, y=y, training_frame=train, validation_frame=valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model performance\n",
    "Let's compare the performance of the two GLMs that were just trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "glm_perf1 = glm_fit1.model_performance(test)\n",
    "glm_perf2 = glm_fit2.model_performance(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ModelMetricsBinomialGLM: glm\n",
      "** Reported on test data. **\n",
      "\n",
      "MSE: 0.142180931463\n",
      "R^2: 0.0596363700308\n",
      "LogLoss: 0.451975547887\n",
      "Null degrees of freedom: 24580\n",
      "Residual degrees of freedom: 24526\n",
      "Null deviance: 23594.8464815\n",
      "Residual deviance: 22220.0218852\n",
      "AIC: 22330.0218852\n",
      "AUC: 0.673463297871\n",
      "Gini: 0.346926595742\n",
      "\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.194176582531: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>13754.0</td>\n",
       "<td>6263.0</td>\n",
       "<td>0.3129</td>\n",
       "<td> (6263.0/20017.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>1970.0</td>\n",
       "<td>2594.0</td>\n",
       "<td>0.4316</td>\n",
       "<td> (1970.0/4564.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>15724.0</td>\n",
       "<td>8857.0</td>\n",
       "<td>0.3349</td>\n",
       "<td> (8233.0/24581.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0      1     Error    Rate\n",
       "-----  -----  ----  -------  ----------------\n",
       "0      13754  6263  0.3129   (6263.0/20017.0)\n",
       "1      1970   2594  0.4316   (1970.0/4564.0)\n",
       "Total  15724  8857  0.3349   (8233.0/24581.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.1941766</td>\n",
       "<td>0.3865584</td>\n",
       "<td>220.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.1125739</td>\n",
       "<td>0.5515444</td>\n",
       "<td>312.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.2733495</td>\n",
       "<td>0.3470929</td>\n",
       "<td>150.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.5242047</td>\n",
       "<td>0.8150197</td>\n",
       "<td>25.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.7302341</td>\n",
       "<td>0.6666667</td>\n",
       "<td>1.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0002607</td>\n",
       "<td>1.0</td>\n",
       "<td>399.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.7351924</td>\n",
       "<td>0.9999500</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_MCC</td>\n",
       "<td>0.1975597</td>\n",
       "<td>0.2076043</td>\n",
       "<td>217.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.1786614</td>\n",
       "<td>0.6247190</td>\n",
       "<td>236.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                      threshold    value     idx\n",
       "--------------------------  -----------  --------  -----\n",
       "max f1                      0.194177     0.386558  220\n",
       "max f2                      0.112574     0.551544  312\n",
       "max f0point5                0.27335      0.347093  150\n",
       "max accuracy                0.524205     0.81502   25\n",
       "max precision               0.730234     0.666667  1\n",
       "max recall                  0.000260656  1         399\n",
       "max specificity             0.735192     0.99995   0\n",
       "max absolute_MCC            0.19756      0.207604  217\n",
       "max min_per_class_accuracy  0.178661     0.624719  236"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gains/Lift Table: Avg response rate: 18.57 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0100077</td>\n",
       "<td>0.4669643</td>\n",
       "<td>2.7586039</td>\n",
       "<td>2.7586039</td>\n",
       "<td>0.5121951</td>\n",
       "<td>0.5121951</td>\n",
       "<td>0.0276074</td>\n",
       "<td>0.0276074</td>\n",
       "<td>175.8603920</td>\n",
       "<td>175.8603920</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0200155</td>\n",
       "<td>0.4288903</td>\n",
       "<td>2.3645176</td>\n",
       "<td>2.5615608</td>\n",
       "<td>0.4390244</td>\n",
       "<td>0.4756098</td>\n",
       "<td>0.0236635</td>\n",
       "<td>0.0512708</td>\n",
       "<td>136.4517646</td>\n",
       "<td>156.1560783</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0300232</td>\n",
       "<td>0.4032008</td>\n",
       "<td>2.3207303</td>\n",
       "<td>2.4812839</td>\n",
       "<td>0.4308943</td>\n",
       "<td>0.4607046</td>\n",
       "<td>0.0232252</td>\n",
       "<td>0.0744961</td>\n",
       "<td>132.0730282</td>\n",
       "<td>148.1283950</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0400309</td>\n",
       "<td>0.3832194</td>\n",
       "<td>2.0361124</td>\n",
       "<td>2.3699911</td>\n",
       "<td>0.3780488</td>\n",
       "<td>0.4400407</td>\n",
       "<td>0.0203769</td>\n",
       "<td>0.0948729</td>\n",
       "<td>103.6112417</td>\n",
       "<td>136.9991067</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0500386</td>\n",
       "<td>0.3677809</td>\n",
       "<td>2.0580061</td>\n",
       "<td>2.3075941</td>\n",
       "<td>0.3821138</td>\n",
       "<td>0.4284553</td>\n",
       "<td>0.0205960</td>\n",
       "<td>0.1154689</td>\n",
       "<td>105.8006099</td>\n",
       "<td>130.7594073</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1000366</td>\n",
       "<td>0.3090170</td>\n",
       "<td>1.8580949</td>\n",
       "<td>2.0829359</td>\n",
       "<td>0.3449959</td>\n",
       "<td>0.3867426</td>\n",
       "<td>0.0929010</td>\n",
       "<td>0.2083699</td>\n",
       "<td>85.8094872</td>\n",
       "<td>108.2935871</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1500346</td>\n",
       "<td>0.2726729</td>\n",
       "<td>1.7090967</td>\n",
       "<td>1.9583566</td>\n",
       "<td>0.3173312</td>\n",
       "<td>0.3636117</td>\n",
       "<td>0.0854514</td>\n",
       "<td>0.2938212</td>\n",
       "<td>70.9096698</td>\n",
       "<td>95.8356602</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2000325</td>\n",
       "<td>0.2487816</td>\n",
       "<td>1.3891889</td>\n",
       "<td>1.8160936</td>\n",
       "<td>0.2579333</td>\n",
       "<td>0.3371975</td>\n",
       "<td>0.0694566</td>\n",
       "<td>0.3632778</td>\n",
       "<td>38.9188855</td>\n",
       "<td>81.6093604</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3000285</td>\n",
       "<td>0.2103532</td>\n",
       "<td>1.3300278</td>\n",
       "<td>1.6540936</td>\n",
       "<td>0.2469487</td>\n",
       "<td>0.3071186</td>\n",
       "<td>0.1329974</td>\n",
       "<td>0.4962752</td>\n",
       "<td>33.0027815</td>\n",
       "<td>65.4093644</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4000244</td>\n",
       "<td>0.1833378</td>\n",
       "<td>1.0890014</td>\n",
       "<td>1.5128349</td>\n",
       "<td>0.2021969</td>\n",
       "<td>0.2808909</td>\n",
       "<td>0.1088957</td>\n",
       "<td>0.6051709</td>\n",
       "<td>8.9001358</td>\n",
       "<td>51.2834939</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5000203</td>\n",
       "<td>0.1613508</td>\n",
       "<td>0.9838262</td>\n",
       "<td>1.4070418</td>\n",
       "<td>0.1826688</td>\n",
       "<td>0.2612481</td>\n",
       "<td>0.0983786</td>\n",
       "<td>0.7035495</td>\n",
       "<td>-1.6173824</td>\n",
       "<td>40.7041795</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.6000163</td>\n",
       "<td>0.1417529</td>\n",
       "<td>0.8194900</td>\n",
       "<td>1.3091231</td>\n",
       "<td>0.1521562</td>\n",
       "<td>0.2430673</td>\n",
       "<td>0.0819457</td>\n",
       "<td>0.7854952</td>\n",
       "<td>-18.0510045</td>\n",
       "<td>30.9123128</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.7000122</td>\n",
       "<td>0.1229728</td>\n",
       "<td>0.6814475</td>\n",
       "<td>1.2194604</td>\n",
       "<td>0.1265256</td>\n",
       "<td>0.2264195</td>\n",
       "<td>0.0681420</td>\n",
       "<td>0.8536372</td>\n",
       "<td>-31.8552470</td>\n",
       "<td>21.9460396</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.8000081</td>\n",
       "<td>0.1033939</td>\n",
       "<td>0.6200953</td>\n",
       "<td>1.1445436</td>\n",
       "<td>0.1151343</td>\n",
       "<td>0.2125095</td>\n",
       "<td>0.0620070</td>\n",
       "<td>0.9156442</td>\n",
       "<td>-37.9904659</td>\n",
       "<td>14.4543574</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.9000041</td>\n",
       "<td>0.0804799</td>\n",
       "<td>0.4513768</td>\n",
       "<td>1.0675285</td>\n",
       "<td>0.0838080</td>\n",
       "<td>0.1982100</td>\n",
       "<td>0.0451358</td>\n",
       "<td>0.9607800</td>\n",
       "<td>-54.8623180</td>\n",
       "<td>6.7528527</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.3922158</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0728234</td>\n",
       "<td>0.1856719</td>\n",
       "<td>0.0392200</td>\n",
       "<td>1.0</td>\n",
       "<td>-60.7784219</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift      cumulative_lift    response_rate    cumulative_response_rate    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  --------  -----------------  ---------------  --------------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.0100077                   0.466964           2.7586    2.7586             0.512195         0.512195                    0.0276074       0.0276074                  175.86    175.86\n",
       "    2        0.0200155                   0.42889            2.36452   2.56156            0.439024         0.47561                     0.0236635       0.0512708                  136.452   156.156\n",
       "    3        0.0300232                   0.403201           2.32073   2.48128            0.430894         0.460705                    0.0232252       0.0744961                  132.073   148.128\n",
       "    4        0.0400309                   0.383219           2.03611   2.36999            0.378049         0.440041                    0.0203769       0.0948729                  103.611   136.999\n",
       "    5        0.0500386                   0.367781           2.05801   2.30759            0.382114         0.428455                    0.020596        0.115469                   105.801   130.759\n",
       "    6        0.100037                    0.309017           1.85809   2.08294            0.344996         0.386743                    0.092901        0.20837                    85.8095   108.294\n",
       "    7        0.150035                    0.272673           1.7091    1.95836            0.317331         0.363612                    0.0854514       0.293821                   70.9097   95.8357\n",
       "    8        0.200033                    0.248782           1.38919   1.81609            0.257933         0.337197                    0.0694566       0.363278                   38.9189   81.6094\n",
       "    9        0.300028                    0.210353           1.33003   1.65409            0.246949         0.307119                    0.132997        0.496275                   33.0028   65.4094\n",
       "    10       0.400024                    0.183338           1.089     1.51283            0.202197         0.280891                    0.108896        0.605171                   8.90014   51.2835\n",
       "    11       0.50002                     0.161351           0.983826  1.40704            0.182669         0.261248                    0.0983786       0.70355                    -1.61738  40.7042\n",
       "    12       0.600016                    0.141753           0.81949   1.30912            0.152156         0.243067                    0.0819457       0.785495                   -18.051   30.9123\n",
       "    13       0.700012                    0.122973           0.681448  1.21946            0.126526         0.226419                    0.068142        0.853637                   -31.8552  21.946\n",
       "    14       0.800008                    0.103394           0.620095  1.14454            0.115134         0.21251                     0.062007        0.915644                   -37.9905  14.4544\n",
       "    15       0.900004                    0.0804799          0.451377  1.06753            0.083808         0.19821                     0.0451358       0.96078                    -54.8623  6.75285\n",
       "    16       1                           2.25236e-19        0.392216  1                  0.0728234        0.185672                    0.03922         1                          -60.7784  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "ModelMetricsBinomialGLM: glm\n",
      "** Reported on test data. **\n",
      "\n",
      "MSE: 0.142186436321\n",
      "R^2: 0.0595999617154\n",
      "LogLoss: 0.452003200537\n",
      "Null degrees of freedom: 24580\n",
      "Residual degrees of freedom: 24524\n",
      "Null deviance: 23594.8464815\n",
      "Residual deviance: 22221.3813448\n",
      "AIC: 22335.3813448\n",
      "AUC: 0.673426207356\n",
      "Gini: 0.346852414711\n",
      "\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.194406524554: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>13784.0</td>\n",
       "<td>6233.0</td>\n",
       "<td>0.3114</td>\n",
       "<td> (6233.0/20017.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>1981.0</td>\n",
       "<td>2583.0</td>\n",
       "<td>0.434</td>\n",
       "<td> (1981.0/4564.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>15765.0</td>\n",
       "<td>8816.0</td>\n",
       "<td>0.3342</td>\n",
       "<td> (8214.0/24581.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0      1     Error    Rate\n",
       "-----  -----  ----  -------  ----------------\n",
       "0      13784  6233  0.3114   (6233.0/20017.0)\n",
       "1      1981   2583  0.434    (1981.0/4564.0)\n",
       "Total  15765  8816  0.3342   (8214.0/24581.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.1944065</td>\n",
       "<td>0.3860987</td>\n",
       "<td>222.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.1076138</td>\n",
       "<td>0.5516607</td>\n",
       "<td>321.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.2744753</td>\n",
       "<td>0.3471057</td>\n",
       "<td>152.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.5243884</td>\n",
       "<td>0.8151011</td>\n",
       "<td>23.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.7273924</td>\n",
       "<td>0.6666667</td>\n",
       "<td>1.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0002627</td>\n",
       "<td>1.0</td>\n",
       "<td>399.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.7346291</td>\n",
       "<td>0.9999500</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_MCC</td>\n",
       "<td>0.1973189</td>\n",
       "<td>0.2074339</td>\n",
       "<td>219.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.1794809</td>\n",
       "<td>0.6270815</td>\n",
       "<td>237.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                      threshold    value     idx\n",
       "--------------------------  -----------  --------  -----\n",
       "max f1                      0.194407     0.386099  222\n",
       "max f2                      0.107614     0.551661  321\n",
       "max f0point5                0.274475     0.347106  152\n",
       "max accuracy                0.524388     0.815101  23\n",
       "max precision               0.727392     0.666667  1\n",
       "max recall                  0.000262715  1         399\n",
       "max specificity             0.734629     0.99995   0\n",
       "max absolute_MCC            0.197319     0.207434  219\n",
       "max min_per_class_accuracy  0.179481     0.627082  237"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gains/Lift Table: Avg response rate: 18.57 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0100077</td>\n",
       "<td>0.4663871</td>\n",
       "<td>2.7804976</td>\n",
       "<td>2.7804976</td>\n",
       "<td>0.5162602</td>\n",
       "<td>0.5162602</td>\n",
       "<td>0.0278265</td>\n",
       "<td>0.0278265</td>\n",
       "<td>178.0497602</td>\n",
       "<td>178.0497602</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0200155</td>\n",
       "<td>0.4273468</td>\n",
       "<td>2.3426240</td>\n",
       "<td>2.5615608</td>\n",
       "<td>0.4349593</td>\n",
       "<td>0.4756098</td>\n",
       "<td>0.0234443</td>\n",
       "<td>0.0512708</td>\n",
       "<td>134.2623964</td>\n",
       "<td>156.1560783</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0300232</td>\n",
       "<td>0.4022065</td>\n",
       "<td>2.2988366</td>\n",
       "<td>2.4739861</td>\n",
       "<td>0.4268293</td>\n",
       "<td>0.4593496</td>\n",
       "<td>0.0230061</td>\n",
       "<td>0.0742770</td>\n",
       "<td>129.8836600</td>\n",
       "<td>147.3986056</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0400309</td>\n",
       "<td>0.3830004</td>\n",
       "<td>1.9923251</td>\n",
       "<td>2.3535708</td>\n",
       "<td>0.3699187</td>\n",
       "<td>0.4369919</td>\n",
       "<td>0.0199387</td>\n",
       "<td>0.0942156</td>\n",
       "<td>99.2325054</td>\n",
       "<td>135.3570805</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0500386</td>\n",
       "<td>0.3670886</td>\n",
       "<td>2.0361124</td>\n",
       "<td>2.2900791</td>\n",
       "<td>0.3780488</td>\n",
       "<td>0.4252033</td>\n",
       "<td>0.0203769</td>\n",
       "<td>0.1145925</td>\n",
       "<td>103.6112417</td>\n",
       "<td>129.0079128</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1000366</td>\n",
       "<td>0.3086472</td>\n",
       "<td>1.8712418</td>\n",
       "<td>2.0807456</td>\n",
       "<td>0.3474369</td>\n",
       "<td>0.3863359</td>\n",
       "<td>0.0935583</td>\n",
       "<td>0.2081507</td>\n",
       "<td>87.1241770</td>\n",
       "<td>108.0745613</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1500346</td>\n",
       "<td>0.2724995</td>\n",
       "<td>1.7090967</td>\n",
       "<td>1.9568962</td>\n",
       "<td>0.3173312</td>\n",
       "<td>0.3633406</td>\n",
       "<td>0.0854514</td>\n",
       "<td>0.2936021</td>\n",
       "<td>70.9096698</td>\n",
       "<td>95.6896232</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2000325</td>\n",
       "<td>0.2485387</td>\n",
       "<td>1.4198649</td>\n",
       "<td>1.8226657</td>\n",
       "<td>0.2636290</td>\n",
       "<td>0.3384177</td>\n",
       "<td>0.0709904</td>\n",
       "<td>0.3645925</td>\n",
       "<td>41.9864949</td>\n",
       "<td>82.2665716</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3000285</td>\n",
       "<td>0.2101283</td>\n",
       "<td>1.3168809</td>\n",
       "<td>1.6540936</td>\n",
       "<td>0.2445077</td>\n",
       "<td>0.3071186</td>\n",
       "<td>0.1316827</td>\n",
       "<td>0.4962752</td>\n",
       "<td>31.6880918</td>\n",
       "<td>65.4093644</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4000244</td>\n",
       "<td>0.1834974</td>\n",
       "<td>1.0911925</td>\n",
       "<td>1.5133827</td>\n",
       "<td>0.2026037</td>\n",
       "<td>0.2809926</td>\n",
       "<td>0.1091148</td>\n",
       "<td>0.6053900</td>\n",
       "<td>9.1192507</td>\n",
       "<td>51.3382671</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5000203</td>\n",
       "<td>0.1614909</td>\n",
       "<td>0.9684881</td>\n",
       "<td>1.4044126</td>\n",
       "<td>0.1798210</td>\n",
       "<td>0.2607599</td>\n",
       "<td>0.0968449</td>\n",
       "<td>0.7022349</td>\n",
       "<td>-3.1511871</td>\n",
       "<td>40.4412629</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.6000163</td>\n",
       "<td>0.1420569</td>\n",
       "<td>0.8194900</td>\n",
       "<td>1.3069321</td>\n",
       "<td>0.1521562</td>\n",
       "<td>0.2426605</td>\n",
       "<td>0.0819457</td>\n",
       "<td>0.7841805</td>\n",
       "<td>-18.0510045</td>\n",
       "<td>30.6932127</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.7000122</td>\n",
       "<td>0.1231478</td>\n",
       "<td>0.7055502</td>\n",
       "<td>1.2210254</td>\n",
       "<td>0.1310008</td>\n",
       "<td>0.2267101</td>\n",
       "<td>0.0705521</td>\n",
       "<td>0.8547327</td>\n",
       "<td>-29.4449825</td>\n",
       "<td>22.1025412</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.8000081</td>\n",
       "<td>0.1037957</td>\n",
       "<td>0.6069484</td>\n",
       "<td>1.1442697</td>\n",
       "<td>0.1126932</td>\n",
       "<td>0.2124587</td>\n",
       "<td>0.0606924</td>\n",
       "<td>0.9154251</td>\n",
       "<td>-39.3051557</td>\n",
       "<td>14.4269694</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.9000041</td>\n",
       "<td>0.0807899</td>\n",
       "<td>0.4535680</td>\n",
       "<td>1.0675285</td>\n",
       "<td>0.0842148</td>\n",
       "<td>0.1982100</td>\n",
       "<td>0.0453550</td>\n",
       "<td>0.9607800</td>\n",
       "<td>-54.6432030</td>\n",
       "<td>6.7528527</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.3922158</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0728234</td>\n",
       "<td>0.1856719</td>\n",
       "<td>0.0392200</td>\n",
       "<td>1.0</td>\n",
       "<td>-60.7784219</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift      cumulative_lift    response_rate    cumulative_response_rate    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  --------  -----------------  ---------------  --------------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.0100077                   0.466387           2.7805    2.7805             0.51626          0.51626                     0.0278265       0.0278265                  178.05    178.05\n",
       "    2        0.0200155                   0.427347           2.34262   2.56156            0.434959         0.47561                     0.0234443       0.0512708                  134.262   156.156\n",
       "    3        0.0300232                   0.402206           2.29884   2.47399            0.426829         0.45935                     0.0230061       0.074277                   129.884   147.399\n",
       "    4        0.0400309                   0.383              1.99233   2.35357            0.369919         0.436992                    0.0199387       0.0942156                  99.2325   135.357\n",
       "    5        0.0500386                   0.367089           2.03611   2.29008            0.378049         0.425203                    0.0203769       0.114592                   103.611   129.008\n",
       "    6        0.100037                    0.308647           1.87124   2.08075            0.347437         0.386336                    0.0935583       0.208151                   87.1242   108.075\n",
       "    7        0.150035                    0.272499           1.7091    1.9569             0.317331         0.363341                    0.0854514       0.293602                   70.9097   95.6896\n",
       "    8        0.200033                    0.248539           1.41986   1.82267            0.263629         0.338418                    0.0709904       0.364592                   41.9865   82.2666\n",
       "    9        0.300028                    0.210128           1.31688   1.65409            0.244508         0.307119                    0.131683        0.496275                   31.6881   65.4094\n",
       "    10       0.400024                    0.183497           1.09119   1.51338            0.202604         0.280993                    0.109115        0.60539                    9.11925   51.3383\n",
       "    11       0.50002                     0.161491           0.968488  1.40441            0.179821         0.26076                     0.0968449       0.702235                   -3.15119  40.4413\n",
       "    12       0.600016                    0.142057           0.81949   1.30693            0.152156         0.242661                    0.0819457       0.784181                   -18.051   30.6932\n",
       "    13       0.700012                    0.123148           0.70555   1.22103            0.131001         0.22671                     0.0705521       0.854733                   -29.445   22.1025\n",
       "    14       0.800008                    0.103796           0.606948  1.14427            0.112693         0.212459                    0.0606924       0.915425                   -39.3052  14.427\n",
       "    15       0.900004                    0.0807899          0.453568  1.06753            0.0842148        0.19821                     0.045355        0.96078                    -54.6432  6.75285\n",
       "    16       1                           2.31756e-19        0.392216  1                  0.0728234        0.185672                    0.03922         1                          -60.7784  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print model performance\n",
    "print glm_perf1\n",
    "print glm_perf2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of printing the entire model performance metrics object, it is probably easier to print just the metric that you are interested in comparing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.673463297871\n",
      "0.673426207356\n"
     ]
    }
   ],
   "source": [
    "# Retreive test set AUC\n",
    "print glm_perf1.auc()\n",
    "print glm_perf2.auc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.676579958645\n",
      "0.671422993628\n"
     ]
    }
   ],
   "source": [
    "# Compare test AUC to the training AUC and validation AUC\n",
    "print glm_fit2.auc(train=True)\n",
    "print glm_fit2.auc(valid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Random Forest\n",
    "H2O's Random Forest (RF) is implements a distributed version of the standard Random Forest algorithm and variable importance measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import H2O RF:\n",
    "from h2o.estimators.random_forest import H2ORandomForestEstimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and a default RF\n",
    "First we will train a basic Random Forest model with default parameters. Random Forest will infer the response distribution from the response encoding. A seed is required for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize the RF estimator:\n",
    "\n",
    "rf_fit1 = H2ORandomForestEstimator(model_id='rf_fit1', seed=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that `rf_fit1` object is initialized, we can train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "drf Model Build Progress: [##################################################] 100%\n"
     ]
    }
   ],
   "source": [
    "rf_fit1.train(x=x, y=y, training_frame=train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train an RF with more trees\n",
    "\n",
    "Next we will increase the number of trees used in the forest by setting `ntrees = 100`.  The default number of trees in an H2O Random Forest is 50, so this RF will be twice as big as the default.  Usually increasing the number of trees in an RF will increase performance as well.  Unlike Gradient Boosting Machines (GBMs), Random Forests are fairly resistant (although not free from) overfitting by increasing the number of trees.  See the GBM example below for additional guidance on preventing overfitting using H2O's early stopping functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "drf Model Build Progress: [##################################################] 100%\n"
     ]
    }
   ],
   "source": [
    "rf_fit2 = H2ORandomForestEstimator(model_id='rf_fit2', ntrees=100, seed=1)\n",
    "rf_fit2.train(x=x, y=y, training_frame=train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare model performance\n",
    "Let's compare the performance of the two RFs that were just trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf_perf1 = rf_fit1.model_performance(test)\n",
    "rf_perf2 = rf_fit2.model_performance(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6650349613\n",
      "0.671842491069\n"
     ]
    }
   ],
   "source": [
    "# Retreive test set AUC\n",
    "print rf_perf1.auc()\n",
    "print rf_perf2.auc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validate performance\n",
    "\n",
    "Rather than using held-out test set to evaluate model performance, a user may wish to estimate model performance using cross-validation.  Using the RF algorithm (with default model parameters) as an example, we demonstrate how to perform k-fold cross-validation using H2O.  No custom code or loops are required, you simply specify the number of desired folds in the `nfolds` argument.\n",
    "\n",
    "Since we are not going to use a test set here, we can use the original (full) dataset, which we called `data` rather than the subsampled `train` dataset.  Note that this will take approximately k (`nfolds`) times longer than training a single RF model, since it will train k models in the cross-validation process (trained on n(k-1)/k rows), in addition to the final model trained on the full `training_frame` dataset with n rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "drf Model Build Progress: [##################################################] 100%\n"
     ]
    }
   ],
   "source": [
    "rf_fit3 = H2ORandomForestEstimator(model_id='rf_fit3', seed=1, nfolds=5)\n",
    "rf_fit3.train(x=x, y=y, training_frame=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the cross-validated AUC, do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.661846394259\n"
     ]
    }
   ],
   "source": [
    "print rf_fit3.auc(xval=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the cross-validated AUC is slighly higher than the test set performance we estimated for `rf_fit1`, and this is likely due to the fact that we trained on more data (n rows) than we did while using `train` as the training set (0.75*n rows) in `rf_fit1`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Gradient Boosting Machine\n",
    "H2O's Gradient Boosting Machine (GBM) offers a Stochastic GBM, which can increase performance quite a bit compared to the original GBM implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import H2O GBM:\n",
    "from h2o.estimators.gbm import H2OGradientBoostingEstimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a default GBM\n",
    "\n",
    "First we will train a basic GBM model with default parameters. GBM will infer the response distribution from the response encoding if not specified explicitly through the `distribution` argument. A seed is required for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "gbm Model Build Progress: [##################################################] 100%\n"
     ]
    }
   ],
   "source": [
    "# Initialize and train the GBM estimator:\n",
    "\n",
    "gbm_fit1 = H2OGradientBoostingEstimator(model_id='gbm_fit1', seed=1)\n",
    "gbm_fit1.train(x=x, y=y, training_frame=train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a GBM with more trees\n",
    "\n",
    "Next we will increase the number of trees used in the GBM by setting `ntrees=500`.  The default number of trees in an H2O GBM is 50, so this GBM will trained using ten times the default.  Increasing the number of trees in a GBM is one way to increase performance of the model, however, you have to be careful not to overfit your model to the training data by using too many trees.  To automatically find the optimal number of trees, you must use H2O's early stopping functionality.  This example will not do that, however, the following example will."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "gbm Model Build Progress: [##################################################] 100%\n"
     ]
    }
   ],
   "source": [
    "gbm_fit2 = H2OGradientBoostingEstimator(model_id='gbm_fit2', ntrees=500, seed=1)\n",
    "gbm_fit2.train(x=x, y=y, training_frame=train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a GBM with early stopping\n",
    "\n",
    "We will again set `ntrees = 500`, however, this time we will use early stopping in order to prevent overfitting (from too many trees).  All of H2O's algorithms have early stopping available, however, with the exception of Deep Learning, it is not enabled by default.  \n",
    "\n",
    "There are several parameters that should be used to control early stopping.  The three that are generic to all the algorithms are: `stopping_rounds`, `stopping_metric` and `stopping_tolerance`.  The stopping metric is the metric by which you'd like to measure performance, and so we will choose AUC here.  The `score_tree_interval` is a parameter specific to Random Forest and GBM.  Setting `score_tree_interval=5` will score the model after every five trees.  The parameters we have set below specify that the model will stop training after there have been three scoring intervals where the AUC has not increased more than 0.0005.  Since we have specified a validation frame, the stopping tolerance will be computed on validation AUC rather than training AUC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "gbm Model Build Progress: [##################################################] 100%\n"
     ]
    }
   ],
   "source": [
    "# Now let's use early stopping to find optimal ntrees\n",
    "\n",
    "gbm_fit3 = H2OGradientBoostingEstimator(model_id='gbm_fit3', \n",
    "                                        ntrees=500, \n",
    "                                        score_tree_interval=5,     #used for early stopping\n",
    "                                        stopping_rounds=3,         #used for early stopping\n",
    "                                        stopping_metric='AUC',     #used for early stopping\n",
    "                                        stopping_tolerance=0.0005, #used for early stopping\n",
    "                                        seed=1)\n",
    "\n",
    "# The use of a validation_frame is recommended with using early stopping\n",
    "gbm_fit3.train(x=x, y=y, training_frame=train, validation_frame=valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare model performance\n",
    "\n",
    "Let's compare the performance of the three GBMs that were just trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gbm_perf1 = gbm_fit1.model_performance(test)\n",
    "gbm_perf2 = gbm_fit2.model_performance(test)\n",
    "gbm_perf3 = gbm_fit3.model_performance(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.682277847572\n",
      "0.6711075658\n",
      "0.683018793141\n"
     ]
    }
   ],
   "source": [
    "# Retreive test set AUC\n",
    "print gbm_perf1.auc()\n",
    "print gbm_perf2.auc()\n",
    "print gbm_perf3.auc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring History\n",
    "\n",
    "To examine the scoring history, use the `scoring_history` method on a trained model.  If `score_tree_interval` is not specified, it will score at various intervals, as we can see for `gbm_fit2.scoring_history()` below.  However, regular 5-tree intervals are used for `gbm_fit3.scoring_history()`.  \n",
    "\n",
    "The `gbm_fit2` was trained only using a training set (no validation set), so the scoring history is calculated for training set performance metrics only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>duration</th>\n",
       "      <th>number_of_trees</th>\n",
       "      <th>training_MSE</th>\n",
       "      <th>training_logloss</th>\n",
       "      <th>training_AUC</th>\n",
       "      <th>training_lift</th>\n",
       "      <th>training_classification_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>2016-05-02 21:30:41</td>\n",
       "      <td>0.003 sec</td>\n",
       "      <td>0</td>\n",
       "      <td>0.148636</td>\n",
       "      <td>0.473845</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.818377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td>2016-05-02 21:30:41</td>\n",
       "      <td>0.077 sec</td>\n",
       "      <td>1</td>\n",
       "      <td>0.147131</td>\n",
       "      <td>0.468892</td>\n",
       "      <td>0.659246</td>\n",
       "      <td>2.381087</td>\n",
       "      <td>0.338662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>2016-05-02 21:30:42</td>\n",
       "      <td>0.137 sec</td>\n",
       "      <td>2</td>\n",
       "      <td>0.145885</td>\n",
       "      <td>0.464902</td>\n",
       "      <td>0.662886</td>\n",
       "      <td>3.007114</td>\n",
       "      <td>0.342909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td>2016-05-02 21:30:42</td>\n",
       "      <td>0.206 sec</td>\n",
       "      <td>3</td>\n",
       "      <td>0.144821</td>\n",
       "      <td>0.461561</td>\n",
       "      <td>0.668455</td>\n",
       "      <td>3.044227</td>\n",
       "      <td>0.347503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td>2016-05-02 21:30:42</td>\n",
       "      <td>0.293 sec</td>\n",
       "      <td>4</td>\n",
       "      <td>0.143939</td>\n",
       "      <td>0.458797</td>\n",
       "      <td>0.672099</td>\n",
       "      <td>3.018472</td>\n",
       "      <td>0.347451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td></td>\n",
       "      <td>2016-05-02 21:30:42</td>\n",
       "      <td>0.390 sec</td>\n",
       "      <td>5</td>\n",
       "      <td>0.143180</td>\n",
       "      <td>0.456460</td>\n",
       "      <td>0.673476</td>\n",
       "      <td>3.032050</td>\n",
       "      <td>0.375141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td></td>\n",
       "      <td>2016-05-02 21:30:42</td>\n",
       "      <td>0.514 sec</td>\n",
       "      <td>6</td>\n",
       "      <td>0.142511</td>\n",
       "      <td>0.454391</td>\n",
       "      <td>0.675946</td>\n",
       "      <td>3.186165</td>\n",
       "      <td>0.359678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td></td>\n",
       "      <td>2016-05-02 21:30:42</td>\n",
       "      <td>0.691 sec</td>\n",
       "      <td>7</td>\n",
       "      <td>0.141913</td>\n",
       "      <td>0.452585</td>\n",
       "      <td>0.678256</td>\n",
       "      <td>3.188863</td>\n",
       "      <td>0.358163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td></td>\n",
       "      <td>2016-05-02 21:30:42</td>\n",
       "      <td>0.892 sec</td>\n",
       "      <td>8</td>\n",
       "      <td>0.141389</td>\n",
       "      <td>0.450968</td>\n",
       "      <td>0.679707</td>\n",
       "      <td>3.319744</td>\n",
       "      <td>0.349209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td></td>\n",
       "      <td>2016-05-02 21:30:43</td>\n",
       "      <td>1.113 sec</td>\n",
       "      <td>9</td>\n",
       "      <td>0.140920</td>\n",
       "      <td>0.449552</td>\n",
       "      <td>0.680947</td>\n",
       "      <td>3.314056</td>\n",
       "      <td>0.343326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td></td>\n",
       "      <td>2016-05-02 21:30:43</td>\n",
       "      <td>1.338 sec</td>\n",
       "      <td>10</td>\n",
       "      <td>0.140523</td>\n",
       "      <td>0.448312</td>\n",
       "      <td>0.682198</td>\n",
       "      <td>3.289249</td>\n",
       "      <td>0.346381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td></td>\n",
       "      <td>2016-05-02 21:30:43</td>\n",
       "      <td>1.582 sec</td>\n",
       "      <td>11</td>\n",
       "      <td>0.140166</td>\n",
       "      <td>0.447230</td>\n",
       "      <td>0.683705</td>\n",
       "      <td>3.274824</td>\n",
       "      <td>0.318943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td></td>\n",
       "      <td>2016-05-02 21:30:43</td>\n",
       "      <td>1.824 sec</td>\n",
       "      <td>12</td>\n",
       "      <td>0.139833</td>\n",
       "      <td>0.446211</td>\n",
       "      <td>0.684975</td>\n",
       "      <td>3.250885</td>\n",
       "      <td>0.309997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td></td>\n",
       "      <td>2016-05-02 21:30:43</td>\n",
       "      <td>2.076 sec</td>\n",
       "      <td>13</td>\n",
       "      <td>0.139540</td>\n",
       "      <td>0.445330</td>\n",
       "      <td>0.686214</td>\n",
       "      <td>3.284399</td>\n",
       "      <td>0.327419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td></td>\n",
       "      <td>2016-05-02 21:30:44</td>\n",
       "      <td>2.343 sec</td>\n",
       "      <td>14</td>\n",
       "      <td>0.139251</td>\n",
       "      <td>0.444442</td>\n",
       "      <td>0.687716</td>\n",
       "      <td>3.337065</td>\n",
       "      <td>0.330116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td></td>\n",
       "      <td>2016-05-02 21:30:44</td>\n",
       "      <td>2.611 sec</td>\n",
       "      <td>15</td>\n",
       "      <td>0.138982</td>\n",
       "      <td>0.443568</td>\n",
       "      <td>0.689360</td>\n",
       "      <td>3.335040</td>\n",
       "      <td>0.339567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td></td>\n",
       "      <td>2016-05-02 21:30:44</td>\n",
       "      <td>2.886 sec</td>\n",
       "      <td>16</td>\n",
       "      <td>0.138745</td>\n",
       "      <td>0.442805</td>\n",
       "      <td>0.691111</td>\n",
       "      <td>3.341852</td>\n",
       "      <td>0.314627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td></td>\n",
       "      <td>2016-05-02 21:30:45</td>\n",
       "      <td>3.185 sec</td>\n",
       "      <td>17</td>\n",
       "      <td>0.138479</td>\n",
       "      <td>0.441970</td>\n",
       "      <td>0.692872</td>\n",
       "      <td>3.343733</td>\n",
       "      <td>0.313904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td></td>\n",
       "      <td>2016-05-02 21:30:45</td>\n",
       "      <td>3.483 sec</td>\n",
       "      <td>18</td>\n",
       "      <td>0.138274</td>\n",
       "      <td>0.441371</td>\n",
       "      <td>0.693973</td>\n",
       "      <td>3.365791</td>\n",
       "      <td>0.309292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td></td>\n",
       "      <td>2016-05-02 21:30:45</td>\n",
       "      <td>3.797 sec</td>\n",
       "      <td>19</td>\n",
       "      <td>0.138091</td>\n",
       "      <td>0.440770</td>\n",
       "      <td>0.694714</td>\n",
       "      <td>3.327489</td>\n",
       "      <td>0.306960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td></td>\n",
       "      <td>2016-05-02 21:30:49</td>\n",
       "      <td>7.836 sec</td>\n",
       "      <td>119</td>\n",
       "      <td>0.131171</td>\n",
       "      <td>0.419460</td>\n",
       "      <td>0.736048</td>\n",
       "      <td>4.203648</td>\n",
       "      <td>0.253120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td></td>\n",
       "      <td>2016-05-02 21:30:59</td>\n",
       "      <td>17.131 sec</td>\n",
       "      <td>337</td>\n",
       "      <td>0.123431</td>\n",
       "      <td>0.397080</td>\n",
       "      <td>0.780090</td>\n",
       "      <td>4.964901</td>\n",
       "      <td>0.220286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td></td>\n",
       "      <td>2016-05-02 21:31:07</td>\n",
       "      <td>26.068 sec</td>\n",
       "      <td>500</td>\n",
       "      <td>0.118563</td>\n",
       "      <td>0.383228</td>\n",
       "      <td>0.804592</td>\n",
       "      <td>5.237803</td>\n",
       "      <td>0.199279</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                timestamp    duration  number_of_trees  training_MSE  \\\n",
       "0     2016-05-02 21:30:41   0.003 sec                0      0.148636   \n",
       "1     2016-05-02 21:30:41   0.077 sec                1      0.147131   \n",
       "2     2016-05-02 21:30:42   0.137 sec                2      0.145885   \n",
       "3     2016-05-02 21:30:42   0.206 sec                3      0.144821   \n",
       "4     2016-05-02 21:30:42   0.293 sec                4      0.143939   \n",
       "5     2016-05-02 21:30:42   0.390 sec                5      0.143180   \n",
       "6     2016-05-02 21:30:42   0.514 sec                6      0.142511   \n",
       "7     2016-05-02 21:30:42   0.691 sec                7      0.141913   \n",
       "8     2016-05-02 21:30:42   0.892 sec                8      0.141389   \n",
       "9     2016-05-02 21:30:43   1.113 sec                9      0.140920   \n",
       "10    2016-05-02 21:30:43   1.338 sec               10      0.140523   \n",
       "11    2016-05-02 21:30:43   1.582 sec               11      0.140166   \n",
       "12    2016-05-02 21:30:43   1.824 sec               12      0.139833   \n",
       "13    2016-05-02 21:30:43   2.076 sec               13      0.139540   \n",
       "14    2016-05-02 21:30:44   2.343 sec               14      0.139251   \n",
       "15    2016-05-02 21:30:44   2.611 sec               15      0.138982   \n",
       "16    2016-05-02 21:30:44   2.886 sec               16      0.138745   \n",
       "17    2016-05-02 21:30:45   3.185 sec               17      0.138479   \n",
       "18    2016-05-02 21:30:45   3.483 sec               18      0.138274   \n",
       "19    2016-05-02 21:30:45   3.797 sec               19      0.138091   \n",
       "20    2016-05-02 21:30:49   7.836 sec              119      0.131171   \n",
       "21    2016-05-02 21:30:59  17.131 sec              337      0.123431   \n",
       "22    2016-05-02 21:31:07  26.068 sec              500      0.118563   \n",
       "\n",
       "    training_logloss  training_AUC  training_lift  \\\n",
       "0           0.473845      0.500000       1.000000   \n",
       "1           0.468892      0.659246       2.381087   \n",
       "2           0.464902      0.662886       3.007114   \n",
       "3           0.461561      0.668455       3.044227   \n",
       "4           0.458797      0.672099       3.018472   \n",
       "5           0.456460      0.673476       3.032050   \n",
       "6           0.454391      0.675946       3.186165   \n",
       "7           0.452585      0.678256       3.188863   \n",
       "8           0.450968      0.679707       3.319744   \n",
       "9           0.449552      0.680947       3.314056   \n",
       "10          0.448312      0.682198       3.289249   \n",
       "11          0.447230      0.683705       3.274824   \n",
       "12          0.446211      0.684975       3.250885   \n",
       "13          0.445330      0.686214       3.284399   \n",
       "14          0.444442      0.687716       3.337065   \n",
       "15          0.443568      0.689360       3.335040   \n",
       "16          0.442805      0.691111       3.341852   \n",
       "17          0.441970      0.692872       3.343733   \n",
       "18          0.441371      0.693973       3.365791   \n",
       "19          0.440770      0.694714       3.327489   \n",
       "20          0.419460      0.736048       4.203648   \n",
       "21          0.397080      0.780090       4.964901   \n",
       "22          0.383228      0.804592       5.237803   \n",
       "\n",
       "    training_classification_error  \n",
       "0                        0.818377  \n",
       "1                        0.338662  \n",
       "2                        0.342909  \n",
       "3                        0.347503  \n",
       "4                        0.347451  \n",
       "5                        0.375141  \n",
       "6                        0.359678  \n",
       "7                        0.358163  \n",
       "8                        0.349209  \n",
       "9                        0.343326  \n",
       "10                       0.346381  \n",
       "11                       0.318943  \n",
       "12                       0.309997  \n",
       "13                       0.327419  \n",
       "14                       0.330116  \n",
       "15                       0.339567  \n",
       "16                       0.314627  \n",
       "17                       0.313904  \n",
       "18                       0.309292  \n",
       "19                       0.306960  \n",
       "20                       0.253120  \n",
       "21                       0.220286  \n",
       "22                       0.199279  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbm_fit2.scoring_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When early stopping is used, we see that training stopped at 105 trees instead of the full 500.  Since we used a validation set in `gbm_fit3`, both training and validation performance metrics are stored in the scoring history object.  Take a look at the validation AUC to observe that the correct stopping tolerance was enforced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>duration</th>\n",
       "      <th>number_of_trees</th>\n",
       "      <th>training_MSE</th>\n",
       "      <th>training_logloss</th>\n",
       "      <th>training_AUC</th>\n",
       "      <th>training_lift</th>\n",
       "      <th>training_classification_error</th>\n",
       "      <th>validation_MSE</th>\n",
       "      <th>validation_logloss</th>\n",
       "      <th>validation_AUC</th>\n",
       "      <th>validation_lift</th>\n",
       "      <th>validation_classification_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>2016-05-02 21:31:21</td>\n",
       "      <td>0.010 sec</td>\n",
       "      <td>0</td>\n",
       "      <td>0.148636</td>\n",
       "      <td>0.473845</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.818377</td>\n",
       "      <td>0.152052</td>\n",
       "      <td>0.481921</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.813013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td>2016-05-02 21:31:22</td>\n",
       "      <td>1.252 sec</td>\n",
       "      <td>5</td>\n",
       "      <td>0.143180</td>\n",
       "      <td>0.456460</td>\n",
       "      <td>0.673476</td>\n",
       "      <td>3.032050</td>\n",
       "      <td>0.375141</td>\n",
       "      <td>0.147481</td>\n",
       "      <td>0.467226</td>\n",
       "      <td>0.654807</td>\n",
       "      <td>2.223073</td>\n",
       "      <td>0.359566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>2016-05-02 21:31:23</td>\n",
       "      <td>1.660 sec</td>\n",
       "      <td>10</td>\n",
       "      <td>0.140523</td>\n",
       "      <td>0.448312</td>\n",
       "      <td>0.682198</td>\n",
       "      <td>3.289249</td>\n",
       "      <td>0.346381</td>\n",
       "      <td>0.145510</td>\n",
       "      <td>0.461083</td>\n",
       "      <td>0.661266</td>\n",
       "      <td>2.468289</td>\n",
       "      <td>0.376873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td>2016-05-02 21:31:23</td>\n",
       "      <td>2.104 sec</td>\n",
       "      <td>15</td>\n",
       "      <td>0.138982</td>\n",
       "      <td>0.443568</td>\n",
       "      <td>0.689360</td>\n",
       "      <td>3.335040</td>\n",
       "      <td>0.339567</td>\n",
       "      <td>0.144536</td>\n",
       "      <td>0.458050</td>\n",
       "      <td>0.664876</td>\n",
       "      <td>2.466610</td>\n",
       "      <td>0.360056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td>2016-05-02 21:31:24</td>\n",
       "      <td>2.623 sec</td>\n",
       "      <td>20</td>\n",
       "      <td>0.137902</td>\n",
       "      <td>0.440160</td>\n",
       "      <td>0.696091</td>\n",
       "      <td>3.375367</td>\n",
       "      <td>0.311120</td>\n",
       "      <td>0.143940</td>\n",
       "      <td>0.456130</td>\n",
       "      <td>0.668252</td>\n",
       "      <td>2.597580</td>\n",
       "      <td>0.344545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td></td>\n",
       "      <td>2016-05-02 21:31:24</td>\n",
       "      <td>3.205 sec</td>\n",
       "      <td>25</td>\n",
       "      <td>0.137022</td>\n",
       "      <td>0.437390</td>\n",
       "      <td>0.701169</td>\n",
       "      <td>3.408881</td>\n",
       "      <td>0.317986</td>\n",
       "      <td>0.143482</td>\n",
       "      <td>0.454688</td>\n",
       "      <td>0.671015</td>\n",
       "      <td>2.553923</td>\n",
       "      <td>0.347606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td></td>\n",
       "      <td>2016-05-02 21:31:25</td>\n",
       "      <td>3.846 sec</td>\n",
       "      <td>30</td>\n",
       "      <td>0.136310</td>\n",
       "      <td>0.435154</td>\n",
       "      <td>0.705434</td>\n",
       "      <td>3.456759</td>\n",
       "      <td>0.316724</td>\n",
       "      <td>0.143138</td>\n",
       "      <td>0.453564</td>\n",
       "      <td>0.673420</td>\n",
       "      <td>2.619408</td>\n",
       "      <td>0.334299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td></td>\n",
       "      <td>2016-05-02 21:31:26</td>\n",
       "      <td>4.525 sec</td>\n",
       "      <td>35</td>\n",
       "      <td>0.135752</td>\n",
       "      <td>0.433369</td>\n",
       "      <td>0.708569</td>\n",
       "      <td>3.619542</td>\n",
       "      <td>0.297379</td>\n",
       "      <td>0.142954</td>\n",
       "      <td>0.452914</td>\n",
       "      <td>0.674670</td>\n",
       "      <td>2.575752</td>\n",
       "      <td>0.343687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td></td>\n",
       "      <td>2016-05-02 21:31:26</td>\n",
       "      <td>5.255 sec</td>\n",
       "      <td>40</td>\n",
       "      <td>0.135230</td>\n",
       "      <td>0.431742</td>\n",
       "      <td>0.711801</td>\n",
       "      <td>3.653056</td>\n",
       "      <td>0.319178</td>\n",
       "      <td>0.142793</td>\n",
       "      <td>0.452352</td>\n",
       "      <td>0.676080</td>\n",
       "      <td>2.575752</td>\n",
       "      <td>0.348014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td></td>\n",
       "      <td>2016-05-02 21:31:27</td>\n",
       "      <td>6.025 sec</td>\n",
       "      <td>45</td>\n",
       "      <td>0.134757</td>\n",
       "      <td>0.430274</td>\n",
       "      <td>0.714796</td>\n",
       "      <td>3.686571</td>\n",
       "      <td>0.311798</td>\n",
       "      <td>0.142660</td>\n",
       "      <td>0.451909</td>\n",
       "      <td>0.677038</td>\n",
       "      <td>2.619408</td>\n",
       "      <td>0.340830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td></td>\n",
       "      <td>2016-05-02 21:31:28</td>\n",
       "      <td>6.905 sec</td>\n",
       "      <td>50</td>\n",
       "      <td>0.134368</td>\n",
       "      <td>0.429098</td>\n",
       "      <td>0.717050</td>\n",
       "      <td>3.763175</td>\n",
       "      <td>0.286432</td>\n",
       "      <td>0.142534</td>\n",
       "      <td>0.451484</td>\n",
       "      <td>0.678057</td>\n",
       "      <td>2.750379</td>\n",
       "      <td>0.341810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td></td>\n",
       "      <td>2016-05-02 21:31:29</td>\n",
       "      <td>7.803 sec</td>\n",
       "      <td>55</td>\n",
       "      <td>0.133973</td>\n",
       "      <td>0.427901</td>\n",
       "      <td>0.719345</td>\n",
       "      <td>3.834991</td>\n",
       "      <td>0.290983</td>\n",
       "      <td>0.142452</td>\n",
       "      <td>0.451223</td>\n",
       "      <td>0.678573</td>\n",
       "      <td>2.663065</td>\n",
       "      <td>0.346259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td></td>\n",
       "      <td>2016-05-02 21:31:30</td>\n",
       "      <td>8.737 sec</td>\n",
       "      <td>60</td>\n",
       "      <td>0.133708</td>\n",
       "      <td>0.427065</td>\n",
       "      <td>0.720801</td>\n",
       "      <td>3.902020</td>\n",
       "      <td>0.281071</td>\n",
       "      <td>0.142403</td>\n",
       "      <td>0.451069</td>\n",
       "      <td>0.678796</td>\n",
       "      <td>2.663065</td>\n",
       "      <td>0.377281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td></td>\n",
       "      <td>2016-05-02 21:31:31</td>\n",
       "      <td>9.720 sec</td>\n",
       "      <td>65</td>\n",
       "      <td>0.133458</td>\n",
       "      <td>0.426308</td>\n",
       "      <td>0.722242</td>\n",
       "      <td>3.911595</td>\n",
       "      <td>0.282794</td>\n",
       "      <td>0.142402</td>\n",
       "      <td>0.451034</td>\n",
       "      <td>0.678852</td>\n",
       "      <td>2.663065</td>\n",
       "      <td>0.374219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td></td>\n",
       "      <td>2016-05-02 21:31:32</td>\n",
       "      <td>10.747 sec</td>\n",
       "      <td>70</td>\n",
       "      <td>0.133182</td>\n",
       "      <td>0.425469</td>\n",
       "      <td>0.723870</td>\n",
       "      <td>3.945109</td>\n",
       "      <td>0.279505</td>\n",
       "      <td>0.142392</td>\n",
       "      <td>0.450987</td>\n",
       "      <td>0.679040</td>\n",
       "      <td>2.684894</td>\n",
       "      <td>0.371770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td></td>\n",
       "      <td>2016-05-02 21:31:33</td>\n",
       "      <td>11.830 sec</td>\n",
       "      <td>75</td>\n",
       "      <td>0.132972</td>\n",
       "      <td>0.424833</td>\n",
       "      <td>0.725203</td>\n",
       "      <td>3.983411</td>\n",
       "      <td>0.263780</td>\n",
       "      <td>0.142379</td>\n",
       "      <td>0.450912</td>\n",
       "      <td>0.679296</td>\n",
       "      <td>2.663065</td>\n",
       "      <td>0.372709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td></td>\n",
       "      <td>2016-05-02 21:31:34</td>\n",
       "      <td>12.973 sec</td>\n",
       "      <td>80</td>\n",
       "      <td>0.132857</td>\n",
       "      <td>0.424458</td>\n",
       "      <td>0.725909</td>\n",
       "      <td>3.983411</td>\n",
       "      <td>0.284578</td>\n",
       "      <td>0.142354</td>\n",
       "      <td>0.450831</td>\n",
       "      <td>0.679525</td>\n",
       "      <td>2.619408</td>\n",
       "      <td>0.368750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td></td>\n",
       "      <td>2016-05-02 21:31:35</td>\n",
       "      <td>14.205 sec</td>\n",
       "      <td>85</td>\n",
       "      <td>0.132571</td>\n",
       "      <td>0.423594</td>\n",
       "      <td>0.727659</td>\n",
       "      <td>4.045652</td>\n",
       "      <td>0.280766</td>\n",
       "      <td>0.142326</td>\n",
       "      <td>0.450719</td>\n",
       "      <td>0.679942</td>\n",
       "      <td>2.663065</td>\n",
       "      <td>0.371403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td></td>\n",
       "      <td>2016-05-02 21:31:36</td>\n",
       "      <td>15.453 sec</td>\n",
       "      <td>90</td>\n",
       "      <td>0.132377</td>\n",
       "      <td>0.423016</td>\n",
       "      <td>0.728880</td>\n",
       "      <td>4.083954</td>\n",
       "      <td>0.272021</td>\n",
       "      <td>0.142337</td>\n",
       "      <td>0.450740</td>\n",
       "      <td>0.680025</td>\n",
       "      <td>2.728550</td>\n",
       "      <td>0.363770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td></td>\n",
       "      <td>2016-05-02 21:31:38</td>\n",
       "      <td>16.777 sec</td>\n",
       "      <td>95</td>\n",
       "      <td>0.132113</td>\n",
       "      <td>0.422232</td>\n",
       "      <td>0.730559</td>\n",
       "      <td>4.112681</td>\n",
       "      <td>0.274640</td>\n",
       "      <td>0.142324</td>\n",
       "      <td>0.450686</td>\n",
       "      <td>0.680095</td>\n",
       "      <td>2.619408</td>\n",
       "      <td>0.370138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td></td>\n",
       "      <td>2016-05-02 21:31:39</td>\n",
       "      <td>18.175 sec</td>\n",
       "      <td>100</td>\n",
       "      <td>0.131908</td>\n",
       "      <td>0.421626</td>\n",
       "      <td>0.731790</td>\n",
       "      <td>4.117469</td>\n",
       "      <td>0.258541</td>\n",
       "      <td>0.142354</td>\n",
       "      <td>0.450739</td>\n",
       "      <td>0.679979</td>\n",
       "      <td>2.597580</td>\n",
       "      <td>0.361321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td></td>\n",
       "      <td>2016-05-02 21:31:41</td>\n",
       "      <td>19.610 sec</td>\n",
       "      <td>105</td>\n",
       "      <td>0.131758</td>\n",
       "      <td>0.421200</td>\n",
       "      <td>0.732602</td>\n",
       "      <td>4.146195</td>\n",
       "      <td>0.266547</td>\n",
       "      <td>0.142323</td>\n",
       "      <td>0.450639</td>\n",
       "      <td>0.680245</td>\n",
       "      <td>2.575752</td>\n",
       "      <td>0.366137</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                timestamp    duration  number_of_trees  training_MSE  \\\n",
       "0     2016-05-02 21:31:21   0.010 sec                0      0.148636   \n",
       "1     2016-05-02 21:31:22   1.252 sec                5      0.143180   \n",
       "2     2016-05-02 21:31:23   1.660 sec               10      0.140523   \n",
       "3     2016-05-02 21:31:23   2.104 sec               15      0.138982   \n",
       "4     2016-05-02 21:31:24   2.623 sec               20      0.137902   \n",
       "5     2016-05-02 21:31:24   3.205 sec               25      0.137022   \n",
       "6     2016-05-02 21:31:25   3.846 sec               30      0.136310   \n",
       "7     2016-05-02 21:31:26   4.525 sec               35      0.135752   \n",
       "8     2016-05-02 21:31:26   5.255 sec               40      0.135230   \n",
       "9     2016-05-02 21:31:27   6.025 sec               45      0.134757   \n",
       "10    2016-05-02 21:31:28   6.905 sec               50      0.134368   \n",
       "11    2016-05-02 21:31:29   7.803 sec               55      0.133973   \n",
       "12    2016-05-02 21:31:30   8.737 sec               60      0.133708   \n",
       "13    2016-05-02 21:31:31   9.720 sec               65      0.133458   \n",
       "14    2016-05-02 21:31:32  10.747 sec               70      0.133182   \n",
       "15    2016-05-02 21:31:33  11.830 sec               75      0.132972   \n",
       "16    2016-05-02 21:31:34  12.973 sec               80      0.132857   \n",
       "17    2016-05-02 21:31:35  14.205 sec               85      0.132571   \n",
       "18    2016-05-02 21:31:36  15.453 sec               90      0.132377   \n",
       "19    2016-05-02 21:31:38  16.777 sec               95      0.132113   \n",
       "20    2016-05-02 21:31:39  18.175 sec              100      0.131908   \n",
       "21    2016-05-02 21:31:41  19.610 sec              105      0.131758   \n",
       "\n",
       "    training_logloss  training_AUC  training_lift  \\\n",
       "0           0.473845      0.500000       1.000000   \n",
       "1           0.456460      0.673476       3.032050   \n",
       "2           0.448312      0.682198       3.289249   \n",
       "3           0.443568      0.689360       3.335040   \n",
       "4           0.440160      0.696091       3.375367   \n",
       "5           0.437390      0.701169       3.408881   \n",
       "6           0.435154      0.705434       3.456759   \n",
       "7           0.433369      0.708569       3.619542   \n",
       "8           0.431742      0.711801       3.653056   \n",
       "9           0.430274      0.714796       3.686571   \n",
       "10          0.429098      0.717050       3.763175   \n",
       "11          0.427901      0.719345       3.834991   \n",
       "12          0.427065      0.720801       3.902020   \n",
       "13          0.426308      0.722242       3.911595   \n",
       "14          0.425469      0.723870       3.945109   \n",
       "15          0.424833      0.725203       3.983411   \n",
       "16          0.424458      0.725909       3.983411   \n",
       "17          0.423594      0.727659       4.045652   \n",
       "18          0.423016      0.728880       4.083954   \n",
       "19          0.422232      0.730559       4.112681   \n",
       "20          0.421626      0.731790       4.117469   \n",
       "21          0.421200      0.732602       4.146195   \n",
       "\n",
       "    training_classification_error  validation_MSE  validation_logloss  \\\n",
       "0                        0.818377        0.152052            0.481921   \n",
       "1                        0.375141        0.147481            0.467226   \n",
       "2                        0.346381        0.145510            0.461083   \n",
       "3                        0.339567        0.144536            0.458050   \n",
       "4                        0.311120        0.143940            0.456130   \n",
       "5                        0.317986        0.143482            0.454688   \n",
       "6                        0.316724        0.143138            0.453564   \n",
       "7                        0.297379        0.142954            0.452914   \n",
       "8                        0.319178        0.142793            0.452352   \n",
       "9                        0.311798        0.142660            0.451909   \n",
       "10                       0.286432        0.142534            0.451484   \n",
       "11                       0.290983        0.142452            0.451223   \n",
       "12                       0.281071        0.142403            0.451069   \n",
       "13                       0.282794        0.142402            0.451034   \n",
       "14                       0.279505        0.142392            0.450987   \n",
       "15                       0.263780        0.142379            0.450912   \n",
       "16                       0.284578        0.142354            0.450831   \n",
       "17                       0.280766        0.142326            0.450719   \n",
       "18                       0.272021        0.142337            0.450740   \n",
       "19                       0.274640        0.142324            0.450686   \n",
       "20                       0.258541        0.142354            0.450739   \n",
       "21                       0.266547        0.142323            0.450639   \n",
       "\n",
       "    validation_AUC  validation_lift  validation_classification_error  \n",
       "0         0.500000         1.000000                         0.813013  \n",
       "1         0.654807         2.223073                         0.359566  \n",
       "2         0.661266         2.468289                         0.376873  \n",
       "3         0.664876         2.466610                         0.360056  \n",
       "4         0.668252         2.597580                         0.344545  \n",
       "5         0.671015         2.553923                         0.347606  \n",
       "6         0.673420         2.619408                         0.334299  \n",
       "7         0.674670         2.575752                         0.343687  \n",
       "8         0.676080         2.575752                         0.348014  \n",
       "9         0.677038         2.619408                         0.340830  \n",
       "10        0.678057         2.750379                         0.341810  \n",
       "11        0.678573         2.663065                         0.346259  \n",
       "12        0.678796         2.663065                         0.377281  \n",
       "13        0.678852         2.663065                         0.374219  \n",
       "14        0.679040         2.684894                         0.371770  \n",
       "15        0.679296         2.663065                         0.372709  \n",
       "16        0.679525         2.619408                         0.368750  \n",
       "17        0.679942         2.663065                         0.371403  \n",
       "18        0.680025         2.728550                         0.363770  \n",
       "19        0.680095         2.619408                         0.370138  \n",
       "20        0.679979         2.597580                         0.361321  \n",
       "21        0.680245         2.575752                         0.366137  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbm_fit3.scoring_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Deep Learning\n",
    "\n",
    "H2O's Deep Learning algorithm is a multilayer feed-forward artificial neural network.  It can also be used to train an autoencoder, however, in the example below we will train a standard supervised prediction model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import H2O DL:\n",
    "from h2o.estimators.deeplearning import H2ODeepLearningEstimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a default DL\n",
    "\n",
    "First we will train a basic DL model with default parameters. DL will infer the response distribution from the response encoding if not specified explicitly through the `distribution` argument. A seed is required for reproducibility.\n",
    "\n",
    "In H2O's DL, early stopping is enabled by default, so below, it will use the training set and default stopping parameters to perform early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "deeplearning Model Build Progress: [##################################################] 100%\n"
     ]
    }
   ],
   "source": [
    "# Initialize and train the DL estimator:\n",
    "\n",
    "dl_fit1 = H2ODeepLearningEstimator(model_id='dl_fit1', seed=1)\n",
    "dl_fit1.train(x=x, y=y, training_frame=train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a DL with new architecture and more epochs\n",
    "\n",
    "Next we will increase the number of epochs used in the GBM by setting `epochs=20` (the default is 10).  Increasing the number of epochs in a deep neural net may increase performance of the model, however, you have to be careful not to overfit your model.  To automatically find the optimal number of epochs, you must use H2O's early stopping functionality.  Unlike the rest of the H2O algorithms, H2O's DL will use early by default, so we will first turn it off in the next example by setting `stopping_rounds=0`, for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "deeplearning Model Build Progress: [##################################################] 100%\n"
     ]
    }
   ],
   "source": [
    "dl_fit2 = H2ODeepLearningEstimator(model_id='dl_fit2', \n",
    "                                   epochs=20, \n",
    "                                   hidden=[10,10], \n",
    "                                   stopping_rounds=0,  #disable early stopping\n",
    "                                   seed=1)\n",
    "dl_fit2.train(x=x, y=y, training_frame=train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a DL with early stopping\n",
    "\n",
    "This example will use the same model parameters as `dl_fit2`, however, we will turn on early stopping and specify the stopping criterion.  We will also pass a validation set, as is recommended for early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "deeplearning Model Build Progress: [##################################################] 100%\n"
     ]
    }
   ],
   "source": [
    "dl_fit3 = H2ODeepLearningEstimator(model_id='dl_fit3', \n",
    "                                   epochs=20, \n",
    "                                   hidden=[10,10],\n",
    "                                   score_interval=1,          #used for early stopping\n",
    "                                   stopping_rounds=3,         #used for early stopping\n",
    "                                   stopping_metric='AUC',     #used for early stopping\n",
    "                                   stopping_tolerance=0.0005, #used for early stopping\n",
    "                                   seed=1)\n",
    "dl_fit3.train(x=x, y=y, training_frame=train, validation_frame=valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare model performance\n",
    "\n",
    "Again, we will compare the model performance of the three models using a test set and AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dl_perf1 = dl_fit1.model_performance(test)\n",
    "dl_perf2 = dl_fit2.model_performance(test)\n",
    "dl_perf3 = dl_fit3.model_performance(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.67102862326\n",
      "0.678929674676\n",
      "0.679735207107\n"
     ]
    }
   ],
   "source": [
    "# Retreive test set AUC\n",
    "print dl_perf1.auc()\n",
    "print dl_perf2.auc()\n",
    "print dl_perf3.auc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>duration</th>\n",
       "      <th>training_speed</th>\n",
       "      <th>epochs</th>\n",
       "      <th>iterations</th>\n",
       "      <th>samples</th>\n",
       "      <th>training_MSE</th>\n",
       "      <th>training_r2</th>\n",
       "      <th>training_logloss</th>\n",
       "      <th>training_AUC</th>\n",
       "      <th>training_lift</th>\n",
       "      <th>training_classification_error</th>\n",
       "      <th>validation_MSE</th>\n",
       "      <th>validation_r2</th>\n",
       "      <th>validation_logloss</th>\n",
       "      <th>validation_AUC</th>\n",
       "      <th>validation_lift</th>\n",
       "      <th>validation_classification_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>2016-05-02 21:33:09</td>\n",
       "      <td>0.000 sec</td>\n",
       "      <td>None</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td>2016-05-02 21:33:09</td>\n",
       "      <td>0.290 sec</td>\n",
       "      <td>486848 rows/sec</td>\n",
       "      <td>0.868510</td>\n",
       "      <td>1</td>\n",
       "      <td>99804</td>\n",
       "      <td>0.144751</td>\n",
       "      <td>0.032042</td>\n",
       "      <td>0.466126</td>\n",
       "      <td>0.666210</td>\n",
       "      <td>2.207266</td>\n",
       "      <td>0.336367</td>\n",
       "      <td>0.147865</td>\n",
       "      <td>0.027352</td>\n",
       "      <td>0.475820</td>\n",
       "      <td>0.662752</td>\n",
       "      <td>2.597580</td>\n",
       "      <td>0.384179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>2016-05-02 21:33:10</td>\n",
       "      <td>1.427 sec</td>\n",
       "      <td>623514 rows/sec</td>\n",
       "      <td>6.961458</td>\n",
       "      <td>8</td>\n",
       "      <td>799969</td>\n",
       "      <td>0.146427</td>\n",
       "      <td>0.020835</td>\n",
       "      <td>0.471884</td>\n",
       "      <td>0.672880</td>\n",
       "      <td>2.207266</td>\n",
       "      <td>0.331210</td>\n",
       "      <td>0.149290</td>\n",
       "      <td>0.017975</td>\n",
       "      <td>0.480727</td>\n",
       "      <td>0.668756</td>\n",
       "      <td>2.728550</td>\n",
       "      <td>0.374342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td>2016-05-02 21:33:11</td>\n",
       "      <td>2.510 sec</td>\n",
       "      <td>693977 rows/sec</td>\n",
       "      <td>13.920123</td>\n",
       "      <td>16</td>\n",
       "      <td>1599617</td>\n",
       "      <td>0.140187</td>\n",
       "      <td>0.062565</td>\n",
       "      <td>0.445000</td>\n",
       "      <td>0.683268</td>\n",
       "      <td>2.538356</td>\n",
       "      <td>0.323018</td>\n",
       "      <td>0.143227</td>\n",
       "      <td>0.057857</td>\n",
       "      <td>0.453731</td>\n",
       "      <td>0.675030</td>\n",
       "      <td>2.663065</td>\n",
       "      <td>0.339891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td>2016-05-02 21:33:12</td>\n",
       "      <td>3.418 sec</td>\n",
       "      <td>734327 rows/sec</td>\n",
       "      <td>20.007832</td>\n",
       "      <td>23</td>\n",
       "      <td>2299180</td>\n",
       "      <td>0.140743</td>\n",
       "      <td>0.058845</td>\n",
       "      <td>0.449854</td>\n",
       "      <td>0.682104</td>\n",
       "      <td>2.538356</td>\n",
       "      <td>0.327164</td>\n",
       "      <td>0.143738</td>\n",
       "      <td>0.054497</td>\n",
       "      <td>0.459053</td>\n",
       "      <td>0.675358</td>\n",
       "      <td>2.859521</td>\n",
       "      <td>0.340871</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               timestamp    duration   training_speed     epochs  iterations  \\\n",
       "0    2016-05-02 21:33:09   0.000 sec             None   0.000000           0   \n",
       "1    2016-05-02 21:33:09   0.290 sec  486848 rows/sec   0.868510           1   \n",
       "2    2016-05-02 21:33:10   1.427 sec  623514 rows/sec   6.961458           8   \n",
       "3    2016-05-02 21:33:11   2.510 sec  693977 rows/sec  13.920123          16   \n",
       "4    2016-05-02 21:33:12   3.418 sec  734327 rows/sec  20.007832          23   \n",
       "\n",
       "   samples  training_MSE  training_r2  training_logloss  training_AUC  \\\n",
       "0        0           NaN          NaN               NaN           NaN   \n",
       "1    99804      0.144751     0.032042          0.466126      0.666210   \n",
       "2   799969      0.146427     0.020835          0.471884      0.672880   \n",
       "3  1599617      0.140187     0.062565          0.445000      0.683268   \n",
       "4  2299180      0.140743     0.058845          0.449854      0.682104   \n",
       "\n",
       "   training_lift  training_classification_error  validation_MSE  \\\n",
       "0            NaN                            NaN             NaN   \n",
       "1       2.207266                       0.336367        0.147865   \n",
       "2       2.207266                       0.331210        0.149290   \n",
       "3       2.538356                       0.323018        0.143227   \n",
       "4       2.538356                       0.327164        0.143738   \n",
       "\n",
       "   validation_r2  validation_logloss  validation_AUC  validation_lift  \\\n",
       "0            NaN                 NaN             NaN              NaN   \n",
       "1       0.027352            0.475820        0.662752         2.597580   \n",
       "2       0.017975            0.480727        0.668756         2.728550   \n",
       "3       0.057857            0.453731        0.675030         2.663065   \n",
       "4       0.054497            0.459053        0.675358         2.859521   \n",
       "\n",
       "   validation_classification_error  \n",
       "0                              NaN  \n",
       "1                         0.384179  \n",
       "2                         0.374342  \n",
       "3                         0.339891  \n",
       "4                         0.340871  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl_fit3.scoring_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Naive Bayes\n",
    "\n",
    "The Naive Bayes (NB) algorithm does not usually beat an algorithm like a Random Forest or GBM, however it is still a popular algorithm, especially in the text domain (when your input is text encoded as \"Bag of Words\", for example).  The Naive Bayes algorithm is for binary or multiclass classification problems only, not regression.  Therefore, your response must be a factor instead of numeric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import H2O NB:\n",
    "from h2o.estimators.naive_bayes import H2ONaiveBayesEstimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a default NB\n",
    "\n",
    "First we will train a basic NB model with default parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "naivebayes Model Build Progress: [##################################################] 100%\n"
     ]
    }
   ],
   "source": [
    "# Initialize and train the NB estimator:\n",
    "\n",
    "nb_fit1 = H2ONaiveBayesEstimator(model_id='nb_fit1')\n",
    "nb_fit1.train(x=x, y=y, training_frame=train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a NB model with Laplace Smoothing\n",
    "\n",
    "One of the few tunable model parameters for the Naive Bayes algorithm is the amount of Laplace smoothing.  The H2O Naive Bayes model will not use any Laplace smoothing by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "naivebayes Model Build Progress: [##################################################] 100%\n"
     ]
    }
   ],
   "source": [
    "nb_fit2 = H2ONaiveBayesEstimator(model_id='nb_fit2', laplace=6)\n",
    "nb_fit2.train(x=x, y=y, training_frame=train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare model performance\n",
    "\n",
    "We will compare the model performance of the two NB models using test set AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_perf1 = nb_fit1.model_performance(test)\n",
    "nb_perf2 = nb_fit2.model_performance(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.648801389108\n",
      "0.649067814706\n"
     ]
    }
   ],
   "source": [
    "# Retreive test set AUC\n",
    "print nb_perf1.auc()\n",
    "print nb_perf2.auc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
